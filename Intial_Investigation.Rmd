---
title: "Intial_Investigation"
author: "Stryder Coleman"
date: "April 14, 2022"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Intial Investigation

Libraries
```{r libraries}
library(readxl)
library(tidyverse)
library(caret)
library(dplyr)
library(stringr)
```
Data Imports
```{r import, include=FALSE}
Summary_Basket_2019 <- read_csv("Summary_Basket_2019.csv")
Summary_Basket_2020 <- read_csv("Summary_Basket_2020.csv")
Summary_Basket_2021 <- read_csv("Summary_Basket_2021.csv")
UPC_Hack_Cereal2 <- read_csv("UPC_Hack_Cereal2.csv")
UPC_Hack_Cleaners2 <- read_csv("UPC_Hack_Cleaners2.csv")
Shopper_Tabs1 <- read_excel("Shopper_Tabs_1.xlsx")
Shopper_Tabs2 <- read_excel("Shopping_Tabs_2.xlsx")
Shopper_Tabs3 <- read_excel("Shopper_Tabs_3.xlsx")

```

#functions
```{r barvar}
#Count the amount occurrences of an attribute
#Index: which attribute
#data: the data set
barVar <- function(data, index = 0) {
    library("tidyverse")
    library("ggpubr")
  
    final_plot = NULL
    #index == 0: plot all attribute counts and combine
    if (index == 0) {
      plots <- vector(mode = "list", length = ncol(data))
      #for each attribute
      for(i in 1:ncol(data)) {
        
        #Count occurences
        temp <- data %>%
        count(.[, i]) %>%
          rename("x" = names(.[1]))

        #plot the occurences
        plots[[i]] <- ggplot(temp,aes(y = n, x = x)) +
        geom_bar(stat = "identity", aes(fill = n)) + 
        theme (axis.text.x = element_text (angle = 45, vjust = 1, hjust=1), 
               legend.position="none") + xlab(names(data[i]))
                
      }
      final_plot <- ggarrange(plotlist = plots, labels = c(1:ncol(data)))
      #index > 0: plot a single attribute count(based on index)
    } else if(index > 0) {
      
        
          #count Occurences
          temp <- data %>%
          count(.[, index])  %>%
            rename("x" = names(.[1]))
      
          #plot the occurences
          final_plot <- #ggplot(temp, aes(y = n,x = names(data[index]))) +
          ggplot(temp, aes(y = n,x = reorder(x,n))) +
          geom_bar(stat = "identity", aes(fill = n)) + 
          theme (axis.text.x = element_text (angle = 45, vjust = 1, hjust=1)) +
            xlab(names(data[index]))
    }
    
    final_plot
}
```

```{r countNA}
countNA <- function(data, graph = FALSE) {
  temp <- tibble(Name = names(data), naAmount = 0)
  for (i in 1:nrow(temp)) {
  temp[i, 2] = sum(is.na(data[i]))
  }
  if(graph == TRUE) {
    ggplot(temp,aes(y = naAmount, x = reorder(Name,-naAmount))) +
    geom_bar(stat = "identity") + 
    theme(axis.text.x = element_text (angle = 45, vjust = 1, hjust=1)) +
      xlab("Attributes")
  } else {
    temp %>%
      arrange(desc(naAmount))
  }
}
```

```{r turkeyOutliers}
# This does the same things as Stryder's Investigate function for outlier's, 
#I just wanted to extract the specific outliers in a table
#------------------------------------------------------------------------------#
#Data: what data to enter
#index: the specific attribute to find outliers for
#Filter_out = TRUE: TRUE = only display non outlying data, 
#                   FALSE = only display outlying data

turkeyOutliers <- function(data, index, filter_out = TRUE) {
  raw_data <- data[index] %>%
    unlist() %>%
    as.vector()
  
  thirdQ <- quantile(raw_data, 3/4)
  firstQ <- quantile(raw_data, 1/4)
  upper <- thirdQ + (1.5 * IQR(raw_data))
  lower <- firstQ - (1.5 * IQR(raw_data))
  
  if(filter_out) {
    data %>%
    filter(data[index] < lower | data[index] > upper)
  } else {
    data %>%
    filter(data[index] >= lower & data[index] <= upper)
  }
      
}


```


# The Investigate function creates three tibbles containing the statistics, the number of outliers per column, and the number of NAs per column

```{r investiongation} 
Investigate <- function(.data, name1, name2, name3)
  {
    numbers <- c()
    names1 <- c()
    for(i in 1:length(.data))
    {
      # Finding numeric columns
      if(is.numeric(.data[[i]]) == TRUE) # Iterates through entire df, column by column
      {
        # Stats section
        names <- append(names, colnames(.data[i])) # getting the names of numeric columns
        mean <- append(mean, mean(.data[[i]], na.rm = TRUE))
        sd <- append(sd, sd(.data[[i]], na.rm = TRUE))
        max <- append(max, max(.data[[i]], na.rm = TRUE))
        min <- append(min, min(.data[[i]], na.rm = TRUE))
        median <- append(median, median(.data[[i]], na.rm = TRUE))
        # Add more stats here
        
        # Finding columns with outliers
        if(length(boxplot.stats(.data[[i]])$out) > 0)
        {
          numbers <- append(numbers, i) 
          names1 <- append(names1, colnames(.data[i])) # collecting the column names of columns containing outliers
        }
      }
    }
      Stats <- tibble(names, mean, median, sd, max, min) # Creating stats data frame
      Stats <- Stats[-1,] # removing junk first row
      view(Stats, title = toString(paste(name1, "Stats", sep = "_"))) # Viewing and renaming table
      NaN_count <- map(.data, ~sum(is.nan(.)))
      na_count <- map(.data, ~sum(is.na(.))) # Counting nas
      names <- as.list(colnames(.data)) #Finding col_names
      unique <- map(.data, ~length(unique(.))) # Finding unique value counts
      Incomplete_data <- tibble(names, na_count, NaN_count, unique) %>%
      arrange(desc(na_count)) # Creating table and organizing it
      view(Incomplete_data, title = toString(paste(name1, "Incomplete_Data", sep = "_"))) # Renaming table and viewing it
      
      outliers <- subset(.data, select = numbers) # Selecting rows that are numeric and contain outliers
      outliers <- map(outliers, ~length(boxplot.stats(.)$out)) # finding count of outliers in each column
      Outliers <- tibble(names1, outliers) %>% 
      rename(names = names1) # Creating table 
      view(Outliers, title = toString(paste(name1, "Outliers", sep = "_"))) # Viewing table and renaming it
  }
```

```{r UPC Analyzer}
upcAnalyze <- function(upc_select, combine = FALSE)  {
  library("tidyverse")
  library("ggpubr")
  if(!exists("full_UPC")) {
    .GlobalEnv$full_UPC <- rbind(UPC_Hack_Cereal2,UPC_Hack_Cleaners2)
  }
  if(class(full_UPC$date) != "Date") {
    full_UPC <- full_UPC %>% 
      mutate(date = as.Date(str_c("20",date)))
  }
  Individual_UPC <- full_UPC %>% #Get the individual Item
    filter(upc == upc_select)
  
  Stores_Bought_From <-unique(Individual_UPC$Store) %>% #get all stores that the item was bought at
    sort()
  
  Most_Common_Store <- Individual_UPC %>%  #Find the most common store the item is bought at
    count(Store) %>% 
    filter(n == max(n))
  
  Average_Units_Bought <- Individual_UPC%>%  #get the average quantity bought of the item
    summarise(mean = mean(qty))
  
  Mean_Dollars_Spent <- Individual_UPC %>%  #get the average price spent of the item
    summarise(mean = mean(price))
  
  
  Price_Of_Item <- Individual_UPC %>%
    ggplot(aes(y = price, x = date)) +
    geom_point() +
    ggtitle("price of item")

  Average_Price_Of_Item <- Individual_UPC %>%
    group_by(date) %>%
    summarise(mean_price = mean(price), average_qty = mean(qty)) %>%
    ggplot(aes(y = mean_price, x = date, color = average_qty)) +
    ylab("Average Price") +
    labs(color='Average Quantity') +
    geom_point() +
    ggtitle("Average price of item")

  distribution <- Individual_UPC %>%
    ggplot(aes(x = price)) +
    geom_boxplot() +
    ggtitle("Distribution")

  if (combine == FALSE) {
    print(Price_Of_Item)
    print(Average_Price_Of_Item)
    print(distribution)
  } else {
    ggarrange(Price_Of_Item, Average_Price_Of_Item,distribution) %>%
      print()
  }
  paste_data <- 
  paste(
    "Stores bought from: ", paste(Stores_Bought_From, collapse = ', ') %>% 
      as.character(), "\n",
    "Most Common Store Bought From: ", paste(Most_Common_Store, collapse = ', ') %>%
    as.character(), "\n",
    "Average Units Bought: ", Average_Units_Bought  %>%
    as.character(), "\n",
    "Average Dollars Spent on product: ", Mean_Dollars_Spent  %>%
    as.character(), "\n")
  writeLines(paste_data)
}
```


```{r}

```


#Remove empty rows
```{r remove_empty_rows}
remove_empty_rows <- function(.source) {
  empty_rows <- apply(.source, 1, function(.source) all(is.na(.source)))
  cleaned <- .source[!empty_rows,]
  return(cleaned)
}
```

#Clean excel imported data
```{r}
Shopper_Tabs1 <- remove_empty_rows(Shopper_Tabs1)
Shopper_Tabs2 <- remove_empty_rows(Shopper_Tabs2)
Shopper_Tabs3 <- remove_empty_rows(Shopper_Tabs3)
view(Shopper_Tabs1)
view(Shopper_Tabs2)
view(Shopper_Tabs3)
```

#Investigate Summary_Basket_2019
```{r}
  #view(Summary_Basket_2019)
  filter(Summary_Basket_2019,Summary_Basket_2019$Online == 1)
  OnlineVStore_2019 <- Summary_Basket_2019 %>% count(Online)

```

#Investigate Summary_Basket_2020
```{r}
  #view(Summary_Basket_2020)
  filter(Summary_Basket_2020,Summary_Basket_2020$Online == 1)
  OnlineVStore_2020 <- Summary_Basket_2020 %>% count(Online)

```

#Investigate Summary_Basket_2021
```{r}
  #view(Summary_Basket_2021)
  filter(Summary_Basket_2021,Summary_Basket_2021$Online == 1) %>%
  rename()
  OnlineVStore_2021 <- Summary_Basket_2021 %>% count(Online)
  

```


Joining 
```{r}
  
  OvR <- left_join(x = OnlineVStore_2019, y = OnlineVStore_2020, by = "Online")
  OvR <- left_join(x=OvR, y = OnlineVStore_2021, by = "Online")
  
  newnames <- c("Online", "2019", "2020", "2021")  
  oldnames <- c("Online", "n.x", "n.y", "n")
  
  OvR <- OvR %>%
    rename_with(~ newnames, all_of(oldnames))
  OvR

```


```{r}
#Investigate(HackDataMaps, "output1","output2","output3")
#Investigate(Summary_Basket_2019, "sum2019","sum2019","sum2019")
#Investigate(Summary_Basket_2020, "sum2020","sum2020","sum2020")
#Investigate(Summary_Basket_2021, "sum2021","sum2021","sum2021")
```

# Visualization


## Summary Baskets Exploration
```{r totdol_vs_totqty_Summary_Basket_2019}
Summary_Basket_2019 %>%
ggplot(aes(y = totdol, x = totqty)) +
  geom_point()

```

* We can see a visible linear relation for totdol and totqty. as the quantity 
bought of a product increases, the total cost of the transaction increase. 

* The visible grouping of points in between x = 0 and x = 500 is most likely due
to shopping behavior where shoppers will buy more of a product if it costs less
or if there is a sale on the product.


```{r mean_totdol_vs_totqty_byCustomer_Summary_Basket_2019_}
Summary_Basket_2019 %>%
  group_by(HHID,Store) %>%
  summarise(mean_totqty = mean(totqty),mean_totdol = mean(totdol)) %>%
  ggplot(aes(y = mean_totdol, x = mean_totqty)) +
  geom_point()

```

* This visualization shows the average quantity bought and average dollar spent per
household. The Visible vertical line of points around x = 0 most likely are expensive
shoppers. These shoppers buy few but expensive products. On the side with points around
y = 0, the shoppers are most likely since they buy a lot of products at a very low price.
In between, these shoppers are the typical shopper where they will spend more if they
buy more or spend less if they buy less. This trend stops at around y = 1600, x = 270.
I believe this is because the typical shopper will not buy more than this amount
and buy more than this amount of product. 

* Tried to see if Store had any correlation with a shoppers spending and found 
no correlation

```{r mean_totdol_vs_totqty_byStore_Summary_Basket_2019_}
Summary_Basket_2019 %>%
  group_by(Store) %>%
  summarise(mean_totqty = mean(totqty),mean_totdol = mean(totdol)) %>%
  ggplot(aes(y = mean_totdol, x = mean_totqty)) +
  geom_point()
```

* This visualization shows two main groups around x = 0,x = 18 and x = 40, x= 55
The left most group are most likely less successful stores since the average 
quantity bought and average dollars spent is less. Meanwhile, for the other group,
these stores were able to convince their costumers to spend more and buy more quantity
at their stores

```{r sum_totdol_vs_totqty_byStore_Summary_Basket_2019_}
Summary_Basket_2019 %>%
  group_by(Store) %>%
  summarise(sum_totqty = sum(totqty),sum_totdol = sum(totdol)) %>%
  ggplot(aes(y = sum_totdol, x = sum_totqty, color = Store)) +
  geom_point()

```
* This visualization shows the total dollars spent in a specific store vs
the total quantity. As one may expect, as the stores' totalqty increases
so does the stores' totaldol. The stores lower left of the visualization are
less popular stores while the stores in the top right are more popular

## UPC Data Exploration

```{r Store Count}
UPC_Hack_Cereal2 %>%
  barVar(index = 2)

UPC_Hack_Cereal2 %>%
  count(Store) %>%
  arrange(desc(n))

UPC_Hack_Cleaners2 %>%
  barVar(index = 2)
 
UPC_Hack_Cleaners2 %>%
  count(Store) %>%
  arrange(desc(n))

```
* Store 15 and Store 17 are the stores with most data available

```{r UPC values}
UPC_Hack_Cereal2 %>%
  group_by(upc)%>%
  nrow()

UPC_Hack_Cereal2 %>%
  group_by(Store)%>%
  nrow()
```
* UPC Values are all the same for every store 

```{r}
  UPC_Hack_Cereal2 %>% 
  group_by(upc) %>%  
  summarise(mean = mean(qty)) %>% 
  arrange(desc(mean))
  
  upcAnalyze(1980070251, combine = FALSE)
  #upcAnalyze(3997800364, combine = TRUE)
  #upcAnalyze(1600027526, combine = FALSE)
```

* Demonstrating UPC Analyzer



